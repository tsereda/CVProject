{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "from typing import List, Dict\n",
    "\n",
    "class PVTFeatureExtractor(nn.Module):\n",
    "    \"\"\"Wrapper for PVT to extract intermediate features\"\"\"\n",
    "    def __init__(self, backbone_name: str = 'pvt_v2_b2'):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(backbone_name, pretrained=True)\n",
    "        \n",
    "        # Remove head\n",
    "        self.model.head = nn.Identity()\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> List[torch.Tensor]:\n",
    "        B = x.shape[0]\n",
    "        \n",
    "        # PVT-v2 forward pass with intermediate features\n",
    "        features = []\n",
    "        \n",
    "        # Patch embedding\n",
    "        x = self.model.patch_embed(x)\n",
    "        if self.model.pos_embed is not None:\n",
    "            x = x + self.model.pos_embed\n",
    "        x = self.model.pos_drop(x)\n",
    "        \n",
    "        # Collect features from each stage\n",
    "        for i in range(len(self.model.blocks)):\n",
    "            x = self.model.blocks[i](x)\n",
    "            if i in [0, 1, 2, 3]:  # Collect features after each stage\n",
    "                # Reshape features to spatial form\n",
    "                H = W = int(x.shape[1] ** 0.5)\n",
    "                features.append(x.reshape(B, H, W, -1).permute(0, 3, 1, 2))\n",
    "        \n",
    "        return features\n",
    "\n",
    "class FPN(nn.Module):\n",
    "    def __init__(self, in_channels_list: List[int], out_channels: int):\n",
    "        super().__init__()\n",
    "        self.lateral_convs = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "            for in_channels in in_channels_list\n",
    "        ])\n",
    "        self.fpn_convs = nn.ModuleList([\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "            for _ in range(len(in_channels_list))\n",
    "        ])\n",
    "\n",
    "    def forward(self, inputs: List[torch.Tensor]) -> List[torch.Tensor]:\n",
    "        # Build laterals\n",
    "        laterals = [\n",
    "            lateral_conv(inputs[i])\n",
    "            for i, lateral_conv in enumerate(self.lateral_convs)\n",
    "        ]\n",
    "        \n",
    "        # Top-down path\n",
    "        used_backbone_levels = len(laterals)\n",
    "        for i in range(used_backbone_levels - 1, 0, -1):\n",
    "            laterals[i - 1] += F.interpolate(\n",
    "                laterals[i],\n",
    "                size=laterals[i - 1].shape[-2:],\n",
    "                mode='nearest'\n",
    "            )\n",
    "        \n",
    "        # Apply FPN convs\n",
    "        outs = [\n",
    "            fpn_conv(laterals[i])\n",
    "            for i, fpn_conv in enumerate(self.fpn_convs)\n",
    "        ]\n",
    "        \n",
    "        return outs\n",
    "\n",
    "class PVTSegmentation(nn.Module):\n",
    "    def __init__(self, backbone_name: str = 'pvt_v2_b2', num_classes: int = 150):\n",
    "        super().__init__()\n",
    "        # Load PVT backbone with feature extraction\n",
    "        self.backbone = PVTFeatureExtractor(backbone_name)\n",
    "        \n",
    "        # Get backbone output channels\n",
    "        dummy_input = torch.randn(1, 3, 512, 512)\n",
    "        features = self.backbone(dummy_input)\n",
    "        in_channels_list = [feat.shape[1] for feat in features]\n",
    "        \n",
    "        # FPN\n",
    "        self.fpn = FPN(in_channels_list, out_channels=256)\n",
    "        \n",
    "        # Segmentation head\n",
    "        self.seg_head = nn.Sequential(\n",
    "            nn.Conv2d(256 * len(in_channels_list), 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, num_classes, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Get features from backbone\n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        # Apply FPN\n",
    "        fpn_features = self.fpn(features)\n",
    "        \n",
    "        # Resize all FPN features to the same size\n",
    "        target_size = fpn_features[0].shape[-2:]\n",
    "        resized_features = [\n",
    "            F.interpolate(feat, size=target_size, mode='bilinear', align_corners=False)\n",
    "            for feat in fpn_features\n",
    "        ]\n",
    "        \n",
    "        # Concatenate features\n",
    "        concat_features = torch.cat(resized_features, dim=1)\n",
    "        \n",
    "        # Apply segmentation head\n",
    "        logits = self.seg_head(concat_features)\n",
    "        \n",
    "        # Resize to input size\n",
    "        output = F.interpolate(\n",
    "            logits,\n",
    "            size=x.shape[-2:],\n",
    "            mode='bilinear',\n",
    "            align_corners=False\n",
    "        )\n",
    "        \n",
    "        return output\n",
    "\n",
    "def create_training_setup(model: nn.Module, learning_rate: float = 1e-4):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
    "    return optimizer, criterion\n",
    "\n",
    "def train_one_epoch(\n",
    "    model: nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    criterion: nn.Module,\n",
    "    device: torch.device\n",
    ") -> float:\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, (images, masks) in enumerate(dataloader):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/path/to/ade20k/images/training'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 145\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 145\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 127\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    124\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# Create dataset and dataloader\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mSimpleADE20K\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/path/to/ade20k\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# Create model (let's start with just ConvNeXt for simplicity)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 22\u001b[0m, in \u001b[0;36mSimpleADE20K.__init__\u001b[0;34m(self, root_dir, split, size)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mannotations\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Get all image files\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfiles \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimg_dir\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfiles) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo images found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/path/to/ade20k/images/training'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class SimpleADE20K(Dataset):\n",
    "    def __init__(self, root_dir, split='train', size=512):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = 'training' if split == 'train' else 'validation'  # ADE20K uses 'training' and 'validation'\n",
    "        self.size = size\n",
    "        \n",
    "        # Setup paths\n",
    "        self.img_dir = os.path.join(root_dir, 'images', self.split)\n",
    "        self.mask_dir = os.path.join(root_dir, 'annotations', self.split)\n",
    "        \n",
    "        # Get all image files\n",
    "        self.files = [f for f in os.listdir(self.img_dir) if f.endswith('.jpg')]\n",
    "        if len(self.files) == 0:\n",
    "            raise ValueError(f\"No images found in {self.img_dir}\")\n",
    "            \n",
    "        print(f\"Found {len(self.files)} images in {self.img_dir}\")\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((size, size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                              std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        self.mask_transform = transforms.Compose([\n",
    "            transforms.Resize((size, size), interpolation=transforms.InterpolationMode.NEAREST),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.files[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        mask_path = os.path.join(self.mask_dir, img_name.replace('.jpg', '.png'))\n",
    "        \n",
    "        # Load image and mask\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        mask = Image.open(mask_path)\n",
    "        \n",
    "        # Apply transforms\n",
    "        image = self.transform(image)\n",
    "        mask = self.mask_transform(mask)\n",
    "        mask = mask.squeeze().long()\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "# Simple segmentation model\n",
    "class SimpleSegmenter(nn.Module):\n",
    "    def __init__(self, backbone_name, num_classes=150):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create backbone\n",
    "        if 'convnext' in backbone_name:\n",
    "            self.backbone = timm.create_model(backbone_name, pretrained=True, \n",
    "                                           features_only=True, out_indices=[3])\n",
    "            feat_dim = self.backbone.feature_info.channels()[-1]\n",
    "        else:  # PVT\n",
    "            self.backbone = timm.create_model(backbone_name, pretrained=True)\n",
    "            self.backbone.head = nn.Identity()  # Remove classification head\n",
    "            feat_dim = 512  # PVT-Small feature dimension\n",
    "        \n",
    "        # Simple segmentation head\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(feat_dim, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, num_classes, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if isinstance(self.backbone.default_cfg['architecture'], str) and 'pvt' in self.backbone.default_cfg['architecture']:\n",
    "            # PVT forward\n",
    "            features = self.backbone(x)\n",
    "            # Reshape from transformer output to spatial features\n",
    "            B = features.shape[0]\n",
    "            H = W = int(np.sqrt(features.shape[1]))\n",
    "            features = features.reshape(B, H, W, -1).permute(0, 3, 1, 2)\n",
    "        else:\n",
    "            # ConvNeXt forward\n",
    "            features = self.backbone(x)[-1]\n",
    "        \n",
    "        # Upsample logits to input resolution\n",
    "        logits = self.head(features)\n",
    "        logits = F.interpolate(logits, size=x.shape[-2:], \n",
    "                             mode='bilinear', align_corners=False)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, (images, masks) in enumerate(dataloader):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def main():\n",
    "    # Setup\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = SimpleADE20K('ADEChallengeData2016', split='train')\n",
    "    dataloader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=4)\n",
    "    \n",
    "    # Create model (let's start with just ConvNeXt for simplicity)\n",
    "    model = SimpleSegmenter('convnext_tiny', num_classes=150)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Training setup\n",
    "    num_epochs = 5\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    # Train\n",
    "    for epoch in range(num_epochs):\n",
    "        loss = train_one_epoch(model, dataloader, criterion, optimizer, device)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss:.4f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
