{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Segmentation with Modern Architectures\n",
    "### Computer Vision Graduate Course Tutorial\n",
    "\n",
    "In this tutorial, we'll explore modern approaches to semantic segmentation by combining:\n",
    "- State-of-the-art backbone architectures (ConvNeXt/PVT/Swin)\n",
    "- Feature Pyramid Network (FPN) decoder\n",
    "- ADE20K dataset\n",
    "\n",
    "## Learning Objectives\n",
    "1. Understand the architecture of modern vision transformers and their use as backbones\n",
    "2. Implement and analyze Feature Pyramid Networks for multi-scale feature fusion\n",
    "3. Build a complete segmentation pipeline using PyTorch best practices\n",
    "4. Gain hands-on experience with real-world dataset handling\n",
    "\n",
    "## Prerequisites\n",
    "- Understanding of CNNs and attention mechanisms\n",
    "- Basic PyTorch knowledge\n",
    "- Familiarity with image segmentation concepts\n",
    "\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Import our modular implementation\n",
    "from seg_model.models import SegmentationModel\n",
    "from seg_model.datasets import ADE20KDataset\n",
    "from seg_model.utils import visualize_batch, calculate_metrics\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Modern Backbones\n",
    "\n",
    "Let's examine the key architectural elements of our available backbones:\n",
    "\n",
    "1. **ConvNeXt**\n",
    "   - Modernized CNN architecture inspired by Vision Transformers\n",
    "   - Uses depthwise convolutions and larger kernels\n",
    "   - Maintains CNN's inductive biases while incorporating transformer benefits\n",
    "\n",
    "2. **PVT (Pyramid Vision Transformer)**\n",
    "   - Hierarchical transformer that generates multi-scale features\n",
    "   - Progressive shrinking of sequence length for efficiency\n",
    "   - Spatial-reduction attention for better memory usage\n",
    "\n",
    "3. **Swin Transformer**\n",
    "   - Hierarchical architecture with shifted windows\n",
    "   - Local self-attention within windows\n",
    "   - Linear complexity with respect to image size\n",
    "\n",
    "### Exercise 1: Backbone Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def analyze_backbone_features(backbone_name: str):\n",
    "    \"\"\"Analyze the feature maps from different backbone stages\"\"\"\n",
    "    model = SegmentationModel(num_classes=150, backbone_name=backbone_name)\n",
    "    \n",
    "    # Create sample input\n",
    "    x = torch.randn(1, 3, 224, 224)\n",
    "    features = model.extract_backbone_features(x)\n",
    "    \n",
    "    # Print feature statistics\n",
    "    print(f\"\\nFeature analysis for {backbone_name}:\")\n",
    "    for idx, feat in enumerate(features):\n",
    "        print(f\"Stage {idx+1}:\")\n",
    "        print(f\"  Shape: {feat.shape}\")\n",
    "        print(f\"  Mean activation: {feat.mean():.4f}\")\n",
    "        print(f\"  Std deviation: {feat.std():.4f}\")\n",
    "\n",
    "# Compare different backbones\n",
    "for backbone in ['convnext_tiny', 'pvt_v2_b0', 'swin_tiny_patch4_window7_224']:\n",
    "    analyze_backbone_features(backbone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion Questions:\n",
    "1. How do the feature map resolutions differ between backbones?\n",
    "2. What are the trade-offs between these architectures?\n",
    "3. Which backbone might be most suitable for segmentation? Why?\n",
    "\n",
    "## Part 2: Feature Pyramid Network Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def visualize_fpn_features(model, image):\n",
    "    \"\"\"Visualize FPN feature maps at different scales\"\"\"\n",
    "    fpn_features = model.extract_fpn_features(image)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, len(fpn_features), figsize=(15, 6))\n",
    "    \n",
    "    for i, feat in enumerate(fpn_features):\n",
    "        # Original feature map\n",
    "        axes[0, i].imshow(feat[0, 0].detach().cpu())\n",
    "        axes[0, i].set_title(f'P{i} Feature Map')\n",
    "        \n",
    "        # Channel-wise attention visualization\n",
    "        attention = feat.mean(dim=1)[0]\n",
    "        axes[1, i].imshow(attention.detach().cpu())\n",
    "        axes[1, i].set_title(f'P{i} Channel Attention')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Load a sample image and visualize features\n",
    "dataset = ADE20KDataset(root_dir='ADEChallengeData2016', split='training')\n",
    "image, _ = dataset[0]\n",
    "model = SegmentationModel(num_classes=150)\n",
    "visualize_fpn_features(model, image.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: FPN Ablation Study\n",
    "\n",
    "Experiment with different FPN configurations:\n",
    "1. Remove lateral connections\n",
    "2. Change upsampling mode\n",
    "3. Modify number of output channels\n",
    "\n",
    "## Part 3: Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Single training epoch with detailed monitoring\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    metrics = {'iou': [], 'pixel_acc': []}\n",
    "    \n",
    "    for batch_idx, (images, masks) in enumerate(train_loader):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        \n",
    "        # Forward pass with intermediate feature extraction\n",
    "        predictions = model(images)\n",
    "        loss = criterion(predictions['p0'], masks)\n",
    "        \n",
    "        # Backward pass with gradient monitoring\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Log gradient statistics\n",
    "        grad_stats = {}\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                grad_stats[name] = {\n",
    "                    'mean': param.grad.mean().item(),\n",
    "                    'std': param.grad.std().item()\n",
    "                }\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        batch_metrics = calculate_metrics(predictions['p0'], masks)\n",
    "        metrics['iou'].append(batch_metrics['iou'])\n",
    "        metrics['pixel_acc'].append(batch_metrics['pixel_acc'])\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Batch {batch_idx}: Loss = {loss.item():.4f}, '\n",
    "                  f'IoU = {batch_metrics[\"iou\"]:.4f}')\n",
    "            \n",
    "    return total_loss / len(train_loader), metrics\n",
    "\n",
    "# Training configuration\n",
    "config = {\n",
    "    'num_epochs': 100,\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 1e-4\n",
    "}\n",
    "\n",
    "# Initialize training components\n",
    "model = SegmentationModel(num_classes=150).to('cuda')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), **config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Topics for Exploration:\n",
    "\n",
    "1. **Loss Functions**\n",
    "   - Implement and compare different segmentation losses\n",
    "   - Combine multiple loss terms\n",
    "   - Analyze class imbalance handling\n",
    "\n",
    "2. **Optimization Strategies**\n",
    "   - Learning rate scheduling\n",
    "   - Gradient accumulation\n",
    "   - Mixed precision training\n",
    "\n",
    "3. **Performance Analysis**\n",
    "   - Error analysis by class\n",
    "   - Confusion matrix visualization\n",
    "   - Boundary accuracy metrics\n",
    "\n",
    "## Assignments\n",
    "\n",
    "1. **Architecture Modification (30 points)**\n",
    "   - Modify the FPN architecture to include attention mechanisms\n",
    "   - Compare performance with baseline\n",
    "   - Analyze computational overhead\n",
    "\n",
    "2. **Dataset Analysis (30 points)**\n",
    "   - Analyze class distribution in ADE20K\n",
    "   - Implement class-balanced sampling\n",
    "   - Evaluate impact on performance\n",
    "\n",
    "3. **Model Analysis (40 points)**\n",
    "   - Conduct ablation studies\n",
    "   - Visualize feature maps and attention\n",
    "   - Compare different backbones\n",
    "   - Write a technical report\n",
    "\n",
    "## Additional Resources\n",
    "- [ConvNeXt Paper](https://arxiv.org/abs/2201.03545)\n",
    "- [PVT Paper](https://arxiv.org/abs/2102.12122)\n",
    "- [Swin Transformer Paper](https://arxiv.org/abs/2103.14030)\n",
    "- [ADE20K Dataset](https://groups.csail.mit.edu/vision/datasets/ADE20K/)"
   ]
  }
 ]
}